% ai, ollama

# Start Ollama server
docker run -d --rm -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Pull LLM model in Ollama
docker exec -it ollama ollama pull <new_model>

# Run LLM model in Ollama
docker exec -it ollama ollama run <model>

# Run LLM fast model in Ollama
docker exec -it ollama ollama run <fast_model>

$ model: docker exec ollama ollama list | awk '{ print $1 }' | grep -v 'NAME'
$ fast_model: echo -e 'tinydolphin\nllama3-gradient\ndeepseek-coder'
